% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/xgb_tune.R
\name{xgb_tune}
\alias{xgb_tune}
\title{Tuning extreme gradient boosting models for domain-level averages}
\usage{
xgb_tune(
  fixed,
  smp_data,
  smp_weights = NULL,
  domains,
  cluster = "domains",
  transformation = "no",
  folds = 10,
  nround = c(150, 250),
  max_depth = c(4, 6),
  colsample_bytree = c(0.6, 1),
  colsample_bylevel = c(0.6, 1),
  colsample_bynode = c(0.6, 1),
  subsample = c(0.6, 1),
  min_child_weight = c(1),
  eta = c(0.3),
  gamma = c(0),
  max_delta_step = c(0),
  lambda = c(1),
  alpha = c(0),
  verbose = FALSE,
  ...
)
}
\arguments{
\item{fixed}{a two-sided linear formula object describing the
fixed-effects part of the model with the dependent variable on the left
of a ~ operator and the explanatory variables on the right, separated
by + operators. All variables (except for \code{domains} and \code{cluster})
must be numeric.}

\item{smp_data}{a data frame that needs to comprise all variables including
\code{domains} and \code{cluster}.}

\item{smp_weights}{a character string containing the name of the variable that
indicates weights in \code{smp_data}. The variable has to be numeric.
Defaults to \code{NULL}.}

\item{domains}{a character string containing the name of a variable
that indicates domains in \code{smp_data}. The variable can be
numeric or a factor.}

\item{cluster}{a character string containing the name of a variable
that indicates clusters in \code{smp_data}. The variable can be
numeric or a factor. Defaults to \code{"domains"}.}

\item{transformation}{a character string. Two different transformation
types for the dependent variable can be chosen (i) no transformation ("no");
(ii) log transformation ("log"); (iii) Arcsin transformation ("arcsin").
Defaults to \code{"no"}.}

\item{folds}{number of folds. Defaults to 10.}

\item{nround}{combination of maximum number of boosting iterations. Defaults to 150 and 250.}

\item{max_depth}{combination of maximum depth of a tree. Defaults to 4 and 6.}

\item{colsample_bytree}{combination of subsample ratios of columns when constructing each tree. Defaults to 0.6 and 1.}

\item{colsample_bylevel}{combination of subsample ratios of columns for each level. Defaults to 0.6 and 1.}

\item{colsample_bynode}{combination of subsample ratios of columns for each node (split). Defaults to 0.6 and 1.}

\item{subsample}{combination of subsample ratios of the training instances. Defaults to 0.6 and 1.}

\item{min_child_weight}{minimum sum of instance weight required in a child node.
If the tree partitioning step produces a leaf node with a sum of instance weight
less than \code{min_child_weight}, then the building process will cease further
partitioning. A larger value of \code{min_child_weight} leads to a more conservative
algorithm. Defaults to 1.}

\item{eta}{step size shrinkage. After each boosting step, one can obtain the
weights of new features directly, and the parameter \code{eta} is used to shrink
these feature weights, thereby making the boosting process more conservative.
Range of [0, 1]. Defaults to 0.3.}

\item{gamma}{minimum loss reduction needed to create an additional partition
on a leaf node of the tree. A larger value of \code{gamma} corresponds to a more
conservative algorithm. Defaults to 0.}

\item{max_delta_step}{maximum allowed step size for adjusting the output of each
leaf. If the value is set to 0, it indicates that there is no constraint Defaults to 0.}

\item{lambda}{L2 regularization term on weights. Increasing this value will result in a more conservative model.
Defaults to 1.}

\item{alpha}{L1 regularization term on weights. Increasing this value will result in a more conservative model.
Defaults to 0.}

\item{verbose}{display progress. Defaults to FALSE.}

\item{...}{additional parameters to be passed to \code{xgboost}.}
}
\value{
An object of class \code{xgb}, \code{emdi}, containing the optimal
hyperparameters for an extreme gradient boosting model.
}
\description{
The funtion \code{xgb_tune} fine-tunes the hyperparameters for extreme gradient boosting models, following \cite{Merfeld and Newhouse (2023)}.
It offers the flexibility to allocate \code{domains} to folds and utilizes estimated
means at the domain-level for cross-validation. Users can specify the number of
folds, including an option for leave-one-out cross-validation.
}
\examples{
\donttest{
# Loading data - population and sample data
data("eusilcA_pop")
data("eusilcA_smp")

xgb_tune_model <- xgb_tune(fixed = eqIncome ~ eqsize + cash + self_empl +
                           unempl_ben + age_ben + surv_ben + sick_ben +
                           dis_ben + rent + fam_allow + house_allow +
                           cap_inv + tax_adj + district,
                           smp_data = eusilcA_smp,
                           domains = "district")
}
}
\references{
Merfeld, J. D., & Newhouse, D. (2023). Improving Estimates of Mean Welfare and Uncertainty
in Developing Countries (No. 10348). The World Bank. \cr \cr
}
